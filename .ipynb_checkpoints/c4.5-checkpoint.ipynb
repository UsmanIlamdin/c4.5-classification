{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from copy import deepcopy\n",
    "\n",
    "# Function: Returns True, if a Column is Numeric\n",
    "def is_number(string):\n",
    "    for i in range(0, len(string)):\n",
    "        if pd.isnull(string[i]) == False:          \n",
    "            try:\n",
    "                float(string[i])\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "\n",
    "# Function: Returns True, if a Value is Numeric\n",
    "def is_number_value(value):\n",
    "    if pd.isnull(value) == False:          \n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "# Function: Performs a Chi_Squared Test or Fisher Exact Test           \n",
    "def chi_squared_test(label_df, feature_df):\n",
    "    label_df.reset_index(drop=True, inplace=True)\n",
    "    feature_df.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([pd.DataFrame(label_df.values.reshape((label_df.shape[0], 1))), feature_df], axis = 1)\n",
    "    data.columns=[\"label\", \"feature\"]\n",
    "    contigency_table = pd.crosstab(data.iloc[:,0], data.iloc[:,1], margins = False)\n",
    "    m = contigency_table.values.sum()\n",
    "    if m <= 10000 and contigency_table.shape == (2,2):\n",
    "        p_value = stats.fisher_exact(contigency_table)\n",
    "    else:\n",
    "        p_value = stats.chi2_contingency(contigency_table, correction = False) # (No Yates' Correction)\n",
    "    return p_value[1]\n",
    "\n",
    "# Function: Prediction           \n",
    "def prediction_dt_c45(model, Xdata):\n",
    "    Xdata = Xdata.reset_index(drop=True)\n",
    "    ydata = pd.DataFrame(index=range(0, Xdata.shape[0]), columns=[\"Prediction\"])\n",
    "    for j in range(0, ydata.shape[1]):\n",
    "        if ydata.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "            for i in range(0, ydata.shape[0]):          \n",
    "               if ydata.iloc[i,j] == 0:\n",
    "                   ydata.iloc[i,j] = \"zero\"\n",
    "               else:\n",
    "                   ydata.iloc[i,j] = \"one\"\n",
    "    data  = pd.concat([ydata, Xdata], axis = 1)\n",
    "    rule = []\n",
    "    \n",
    "    # Preprocessing - Boolean Values\n",
    "    for j in range(0, data.shape[1]):\n",
    "        if data.iloc[:,j].dtype == \"bool\":\n",
    "            data.iloc[:,j] = data.iloc[:, j].astype(str)\n",
    "                   \n",
    "    dt_model = deepcopy(model)\n",
    "    \n",
    "    for i in range(0, len(dt_model)):\n",
    "        dt_model[i] = dt_model[i].replace(\"{\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"}\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\".\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"IF \", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"AND\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"THEN\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"=\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"<\", \"<=\")\n",
    "    \n",
    "    for i in range(0, len(dt_model) -2): \n",
    "        splited_rule = [x for x in dt_model[i].split(\" \") if x]\n",
    "        rule.append(splited_rule)\n",
    "   \n",
    "    for i in range(0, Xdata.shape[0]): \n",
    "        for j in range(0, len(rule)):\n",
    "            rule_confirmation = len(rule[j])/2 - 1\n",
    "            rule_count = 0\n",
    "            for k in range(0, len(rule[j]) - 2, 2):\n",
    "                if is_number_value(data[rule[j][k]][i]) == False:\n",
    "                    if (data[rule[j][k]][i] in rule[j][k+1]):\n",
    "                        rule_count = rule_count + 1\n",
    "                        if (rule_count == rule_confirmation):\n",
    "                            data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                    else:\n",
    "                        k = len(rule[j])\n",
    "                elif is_number_value(data[rule[j][k]][i]) == True:\n",
    "                     if rule[j][k+1].find(\"<=\") == 0:\n",
    "                         if data[rule[j][k]][i] <= float(rule[j][k+1].replace(\"<=\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "                     elif rule[j][k+1].find(\">\") == 0:\n",
    "                         if data[rule[j][k]][i] > float(rule[j][k+1].replace(\">\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "    \n",
    "    for i in range(0, Xdata.shape[0]):\n",
    "        if pd.isnull(data.iloc[i,0]):\n",
    "            data.iloc[i,0] = dt_model[len(dt_model)-1]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function: Calculates the Information Gain Ratio  \n",
    "def info_gain_ratio(target, feature = [], uniques = []):\n",
    "    entropy = 0\n",
    "    denominator_1 = feature.count()\n",
    "    data = pd.concat([pd.DataFrame(target.values.reshape((target.shape[0], 1))), feature], axis = 1)\n",
    "    for entp in range(0, len(np.unique(target))):\n",
    "        numerator_1 = data.iloc[:,0][(data.iloc[:,0] == np.unique(target)[entp])].count()\n",
    "        if numerator_1 > 0:\n",
    "            entropy = entropy - (numerator_1/denominator_1)* np.log2((numerator_1/denominator_1))\n",
    "    info_gain = float(entropy)\n",
    "    info_gain_r = 0\n",
    "    intrinsic_v = 0\n",
    "    for word in range(0, len(uniques)):\n",
    "        denominator_2 = feature[(feature == uniques[word])].count()\n",
    "        if denominator_2[0] > 0:\n",
    "            intrinsic_v = intrinsic_v - (denominator_2/denominator_1)* np.log2((denominator_2/denominator_1))\n",
    "        for lbl in range(0, len(np.unique(target))):\n",
    "            numerator_2 = data.iloc[:,0][(data.iloc[:,0] == np.unique(target)[lbl]) & (data.iloc[:,1]  == uniques[word])].count()\n",
    "            if numerator_2 > 0:\n",
    "                info_gain = info_gain + (denominator_2/denominator_1)*(numerator_2/denominator_2)* np.log2((numerator_2/denominator_2))\n",
    "    if intrinsic_v[0] > 0:\n",
    "        info_gain_r = info_gain/intrinsic_v\n",
    "    return float(info_gain_r)\n",
    "\n",
    "# Function: Binary Split on Continuous Variables \n",
    "def split_me(feature, split):\n",
    "    result = pd.DataFrame(feature.values.reshape((feature.shape[0], 1)))\n",
    "    for fill in range(0, len(feature)):\n",
    "        result.iloc[fill,0] = feature.iloc[fill]\n",
    "    lower = \"<=\" + str(split)\n",
    "    upper = \">\" + str(split)\n",
    "    for convert in range(0, len(feature)):\n",
    "        if float(feature.iloc[convert]) <= float(split):\n",
    "            result.iloc[convert,0] = lower\n",
    "        else:\n",
    "            result.iloc[convert,0] = upper\n",
    "    binary_split = []\n",
    "    binary_split = [lower, upper]\n",
    "    return result, binary_split\n",
    "\n",
    "# Function: C4.5 Algorithm\n",
    "def dt_c45(Xdata, ydata, cat_missing = \"none\", num_missing = \"none\", pre_pruning = \"none\", chi_lim = 0.1, min_lim = 5):\n",
    "    \n",
    "    ################     Part 1 - Preprocessing    #############################\n",
    "    # Preprocessing - Creating Dataframe\n",
    "    name = ydata.name\n",
    "    ydata = pd.DataFrame(ydata.values.reshape((ydata.shape[0], 1)))\n",
    "    for j in range(0, ydata.shape[1]):\n",
    "        if ydata.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "            for i in range(0, ydata.shape[0]):          \n",
    "               if ydata.iloc[i,j] == 0:\n",
    "                   ydata.iloc[i,j] = \"zero\"\n",
    "               else:\n",
    "                   ydata.iloc[i,j] = \"one\"\n",
    "    dataset = pd.concat([ydata, Xdata], axis = 1)\n",
    "    \n",
    "     # Preprocessing - Boolean Values\n",
    "    for j in range(0, dataset.shape[1]):\n",
    "        if dataset.iloc[:,j].dtype == \"bool\":\n",
    "            dataset.iloc[:,j] = dataset.iloc[:, j].astype(str)\n",
    "\n",
    "    # Preprocessing - Missing Values\n",
    "    if cat_missing != \"none\":\n",
    "        for j in range(1, dataset.shape[1]): \n",
    "            if is_number(dataset.iloc[:, j]) == False:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if cat_missing == \"missing\":\n",
    "                            dataset.iloc[i,j] = \"Unknow\"\n",
    "                        elif cat_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif cat_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]            \n",
    "    elif num_missing != \"none\":\n",
    "            if is_number(dataset.iloc[:, j]) == True:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if num_missing == \"mean\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].mean()\n",
    "                        elif num_missing == \"median\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].median()\n",
    "                        elif num_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif num_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]  \n",
    "    \n",
    "    # Preprocessing - Unique Words List\n",
    "    unique = []\n",
    "    uniqueWords = []\n",
    "    for j in range(0, dataset.shape[1]): \n",
    "        for i in range(0, dataset.shape[0]):\n",
    "            token = dataset.iloc[i, j]\n",
    "            if not token in unique:\n",
    "                unique.append(token)\n",
    "        uniqueWords.append(unique)\n",
    "        unique = []  \n",
    "    \n",
    "    # Preprocessing - Label Matrix\n",
    "    label = np.array(uniqueWords[0])\n",
    "    label = label.reshape(1, len(uniqueWords[0]))\n",
    "    \n",
    "    ################    Part 2 - Initialization    #############################\n",
    "    # C4.5 - Initializing Variables\n",
    "    i = 0\n",
    "    impurity = 0\n",
    "    branch = [None]*1\n",
    "    branch[0] = dataset\n",
    "    gain_ratio = np.empty([1, branch[i].shape[1]])\n",
    "    lower = \"0\"\n",
    "    root_index = 0\n",
    "    rule = [None]*1\n",
    "    rule[0] = \"IF \"\n",
    "    skip_update = False\n",
    "    stop = 2\n",
    "    upper = \"1\"\n",
    "    \n",
    "    ################     Part 3 - C4.5 Algorithm    #############################\n",
    "    # C4.5 - Algorithm\n",
    "    while (i < stop):\n",
    "        impurity = np.amax(gain_ratio)\n",
    "        gain_ratio.fill(0)\n",
    "        for element in range(1, branch[i].shape[1]):\n",
    "            if len(branch[i]) == 0:\n",
    "                skip_update = True \n",
    "                break\n",
    "            if len(np.unique(branch[i][0])) == 1 or len(branch[i]) == 1:\n",
    "                 if \".\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].iloc[0, 0] + \".\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 break\n",
    "            if i > 0 and is_number(dataset.iloc[:, element]) == False and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], branch[i].iloc[:, element]) > chi_lim:\n",
    "                 if \".\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \".\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "            if is_number(dataset.iloc[:, element]) == True:\n",
    "                gain_ratio[0, element] = 0.0\n",
    "                value = np.sort(branch[i].iloc[:, element].unique())\n",
    "                skip_update = False\n",
    "                if branch[i][(branch[i].iloc[:, element] == value[0])].count()[0] > 1:\n",
    "                    start = 0\n",
    "                    finish = len(branch[i].iloc[:, element].unique()) - 2\n",
    "                else:\n",
    "                    start = 1\n",
    "                    finish = len(branch[i].iloc[:, element].unique()) - 2\n",
    "                if len(branch[i]) == 2 or len(value) == 1 or len(value) == 2:\n",
    "                    start = 0\n",
    "                    finish = 1\n",
    "                if len(value) == 3:\n",
    "                    start = 0\n",
    "                    finish = 2\n",
    "                for bin_split in range(start, finish):\n",
    "                    bin_sample = split_me(feature = branch[i].iloc[:, element], split = value[bin_split])\n",
    "                    if i > 0 and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], bin_sample[0]) > chi_lim:\n",
    "                        if \".\" not in rule[i]:\n",
    "                             rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \".\"\n",
    "                             rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                        skip_update = True\n",
    "                        continue\n",
    "                    igr = info_gain_ratio(target = branch[i].iloc[:, 0], feature = bin_sample[0], uniques = bin_sample[1])\n",
    "                    if igr > float(gain_ratio[0, element]):\n",
    "                        gain_ratio[0, element] = igr\n",
    "                        uniqueWords[element] = bin_sample[1]\n",
    "            if is_number(dataset.iloc[:, element]) == False:\n",
    "                gain_ratio[0, element] = 0.0\n",
    "                skip_update = False\n",
    "                igr = info_gain_ratio(target = branch[i].iloc[:, 0], feature =  pd.DataFrame(branch[i].iloc[:, element].values.reshape((branch[i].iloc[:, element].shape[0], 1))), uniques = uniqueWords[element])\n",
    "                gain_ratio[0, element] = igr\n",
    "            if i > 0 and pre_pruning == \"min\" and len(branch[i]) <= min_lim:\n",
    "                 if \".\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \".\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "           \n",
    "        if i > 0 and pre_pruning == \"impur\" and np.amax(gain_ratio) < impurity and np.amax(gain_ratio) > 0:\n",
    "             if \".\" not in rule[i]:\n",
    "                 rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \".\"\n",
    "                 rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "             skip_update = True\n",
    "             continue\n",
    "        \n",
    "        if skip_update == False:\n",
    "            root_index = np.argmax(gain_ratio)\n",
    "            rule[i] = rule[i] + str(list(branch[i])[root_index])\n",
    "            \n",
    "            for word in range(0, len(uniqueWords[root_index])):\n",
    "                uw = uniqueWords[root_index][word].replace(\"<=\", \"\")\n",
    "                uw = uw.replace(\">\", \"\")\n",
    "                lower = \"<=\" + uw\n",
    "                upper = \">\" + uw\n",
    "                if uniqueWords[root_index][word] == lower:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] <= float(uw)])\n",
    "                elif uniqueWords[root_index][word] == upper:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index]  > float(uw)])\n",
    "                else:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] == uniqueWords[root_index][word]])\n",
    "                \n",
    "                rule.append(rule[i] + \" = \" + \"{\" + uniqueWords[root_index][word] + \"}\")\n",
    "            \n",
    "            for logic_connection in range(1, len(rule)):\n",
    "                if len(np.unique(branch[i][0])) != 1 and rule[logic_connection].endswith(\" AND \") == False  and rule[logic_connection].endswith(\"}\") == True:\n",
    "                    rule[logic_connection] = rule[logic_connection] + \" AND \"\n",
    "        skip_update = False\n",
    "        i = i + 1\n",
    "        print(\"iteration: \", i)\n",
    "        stop = len(rule)\n",
    "    \n",
    "    for i in range(len(rule) - 1, -1, -1):\n",
    "        if rule[i].endswith(\".\") == False:\n",
    "            del rule[i]    \n",
    "\n",
    "    rule.append(\"Total Number of Rules: \" + str(len(rule)))\n",
    "    rule.append(dataset.agg(lambda x:x.value_counts().index[0])[0])\n",
    "    print(\"End of Iterations\")\n",
    "    \n",
    "    return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank-additional/bank-additional/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:20]\n",
    "y = df.iloc[:, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt_c45(Xdata = X, ydata = y, cat_missing = \"none\", num_missing = \"none\", pre_pruning = \"impur\", chi_lim = 0.1, min_lim = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  df.iloc[0:2, 0:4]\n",
    "prediction_dt_c45(dt_model, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
